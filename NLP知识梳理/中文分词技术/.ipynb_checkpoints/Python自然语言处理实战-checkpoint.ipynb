{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 规则分词\n",
    "\n",
    "- 一种机械的分词方法，主要通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到着切分，否则不予切分\n",
    "\n",
    "## 分词算法\n",
    "\n",
    "### 正向匹配算法\n",
    "\n",
    "- 基本思想\n",
    "\n",
    "假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字符串中的前i个作为匹配字段，查找词典。\n",
    "\n",
    "> 若词典中，存在这样的一个i字词，则匹配成功\n",
    "\n",
    "> 否则，去掉匹配字段中的最后一个字，重新进行查找\n",
    "\n",
    "### 逆向匹配算法\n",
    "\n",
    "- 基本思想\n",
    "\n",
    "从被处理文档的末端开始匹配扫描，每次取最末端的第i个字符（i同样也为词典中最长词数）作为匹配字段，\n",
    "\n",
    "> 若字典中，存在这样的一个i字词，则匹配成功\n",
    "\n",
    "> 否则，去掉匹配字段最前面的一个字，继续匹配\n",
    "\n",
    "相应的，它使用的分词词典，是逆序词典\n",
    "\n",
    "### 双向匹配算法\n",
    "\n",
    "- 基本思想\n",
    "\n",
    "将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果\n",
    "\n",
    "\n",
    "## 统计分词\n",
    "\n",
    "### 语言模型\n",
    "\n",
    "- n元语法模型及思想\n",
    "\n",
    "### HMM模型\n",
    "\n",
    "- 概念及思想\n",
    "\n",
    "- 计算方法 ： Veterbi算法\n",
    "\n",
    "## 中文分词工具 - jieba\n",
    "\n",
    "### 精确模式\n",
    "\n",
    "- 试图将句子最精确地切开，适合文本分析\n",
    "\n",
    "### 全模式\n",
    "\n",
    "- 把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义\n",
    "\n",
    "### 搜索引擎模式\n",
    "\n",
    "- 在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎\n",
    "\n",
    "## 实战之高频词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 命名实体识别与词性标注\n",
    "\n",
    "## 词性标注\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 在给定句子中判断每个词的语法范畴，确定其词性并加以标注的过程\n",
    "\n",
    "### 词性标注规范\n",
    "\n",
    "- 中文领域中尚无统一的标注标准，较为主流的主要为北大的词性标注集和宾州词性标注集\n",
    "\n",
    "### jieba分词中的词性标注\n",
    "\n",
    "- 在词性标注任务中，Jieba分词采用了simultaneous思想的联合模型方法，即将基于字标注的分词方法与词性标注结合起来，使用复合标注集。【P62】\n",
    "\n",
    "## 命名实体识别(Named Entities Recognition, NER)\n",
    "\n",
    "### 分类\n",
    "\n",
    "-  NER研究的命名实体一般分为3大类和7小类\n",
    "\n",
    "> 3大类指的是：实体类、时间类和数字类\n",
    "\n",
    "> 7小类指的是：人名、地名、组织机构名、时间、日期、货币、百分比\n",
    "\n",
    "其中，人名、地名、机构名相对较为复杂，是近年来的主要研究方向，而其他类别通常可以采用模式匹配的方式获得较好的识别效果。\n",
    "\n",
    "### 一些特点\n",
    "\n",
    "- 命名实体识别更侧重高召回率，但是在信息检索领域，高准确率更重要\n",
    "\n",
    "- 命名实体识别效果的评判，主要看实体的边界是否划分正确，以及实体的类型是否标注正确\n",
    "\n",
    "### 困难点\n",
    "\n",
    "- 各类命名实体的数量众多\n",
    "\n",
    "- 命名实体的构成规律复杂\n",
    "\n",
    "- 嵌套情况复杂\n",
    "\n",
    "- 长度不确定\n",
    "\n",
    "\n",
    "### 识别方法\n",
    "\n",
    "- 基于规则的方法\n",
    "\n",
    "> 1. 规则依赖具体语言、领域和文本风格\n",
    "\n",
    "> 2. 规则编制过程耗时且难以覆盖所有语言现象\n",
    "\n",
    "> 3. 可移植性差、更新维护困难\n",
    "\n",
    "- 基于统计的方法\n",
    "\n",
    "> 1. 主要思想：\n",
    "\n",
    "> 基于人工标注的语料，将命名实体识别任务作为序列标注问题来解决\n",
    "\n",
    "【什么叫序列标注任务?】\n",
    "\n",
    "> 2. 主流的基于统计的命名实体识别方法\n",
    "\n",
    "> a. 隐马尔科夫模型\n",
    "\n",
    "> b. 最大熵模型\n",
    "\n",
    "> c. 条件随机场\n",
    "\n",
    "> 3. 特点\n",
    "\n",
    "> 基于统计的方法对语料的依赖比较大，而可以用来建设和评估识别系统的大规模通用语料库又比较少，这是该方法的一大制约。\n",
    "\n",
    "- 混合方法\n",
    "\n",
    "> 在很多情况下是使用混合方法，结合规则和统计方法\n",
    "\n",
    "\n",
    "### 基于条件随机场的命名实体识别\n",
    "\n",
    "- 序列标注方式是目前命名实体识别中的主流方法，HMM方法之前已经介绍过，这里主要介绍的是条件随机场【CRF】,需要搞明白其原理，及适用场景等。\n",
    "\n",
    "\n",
    "## 实战\n",
    "\n",
    "### 日期识别\n",
    "\n",
    "### 地名识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键字提取算法\n",
    "\n",
    "## TF/IDF算法\n",
    "\n",
    "### 基本思想\n",
    "\n",
    "- TF-IDF算法由两部分组成\n",
    "\n",
    "> TF算法是统计一个词在一篇文章中出现的频次，其基本思想是，一个词在文档中出现的次数越多，则其对文档的表达能力也就越强。\n",
    "\n",
    "> 而IDF算法则是统计一个词在文档集的多少个文档中出现，其基本思想是，如果一个词在越少的文档中出现，则其对文档的区分能力也就越强。\n",
    "\n",
    "### 计算公式\n",
    "\n",
    "> TF算法公式\n",
    "$$tf_{ij} = \\frac{ n_{ij} }{ \\sum_{k}{n_{ij}} }$$\n",
    "\n",
    "- 其中，分子表示词**i**在文档**j**中的出现频次，但是仅用频次来表示，长文本中的词出现频次高的概率会更大，这一点会影响到不同文档之间关键词权值的比较，所以在计算的过程中一般会对词频进行归一化。\n",
    "\n",
    "- 分母表示文档中，每个词出现次数的总和，也就是文档的总次数。\n",
    "\n",
    "> IDF算法公式\n",
    "\n",
    "\\begin{equation} idf_i = log \\begin{cases}   a+b  \\end{cases}\\end{equation}\n",
    "\n",
    "- 这个公式暂时没表达出来\n",
    "\n",
    "> TF-IDF算法就是TF算法与IDF算法的总和使用\n",
    "\n",
    "$$tf * idf(i,j) = tf_{ij} * idf_i$$\n",
    "\n",
    "\n",
    "## TextRank算法\n",
    "\n",
    "### 算法特点\n",
    "\n",
    "- TextRank算法与其他算法不同的一点是，其他算法的关键词提取都要基于一个现成的语料库, 如：\n",
    "\n",
    "> TF-IDF中需要统计每个词在语料库中的多少个文档有出现过，也就是逆文档频率。\n",
    "\n",
    "> 主题模型的关键词提取算法则是要通过对大规模文档的学习，来发现文档的隐含主题\n",
    "\n",
    "而TextRank算法则可以脱离语料库的背景，仅对单篇文档进行分析就可以提取该文档的关键词。\n",
    "\n",
    "\n",
    "### 算法思想\n",
    "\n",
    "TextRank算法的基本思想来源于Google的PageRank算法。\n",
    "\n",
    "### PageRank算法介绍\n",
    "\n",
    "- 算法创始人\n",
    "\n",
    "拉里·佩奇和谢尔盖·布林【两位于1997年构建早期的搜素系统原型时提出的链接分析法】\n",
    "\n",
    "- 基本思想【其基本思想有两条】\n",
    "\n",
    "> 1. 连接数量：一个网页被越多的其他网页链接，说明这个网页越重要\n",
    "\n",
    "> 2. 链接质量：一个网页被一个越高权值的网页链接，也能表明这个网页越重要\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "\n",
    "## 主题模型算法\n",
    "\n",
    "### 提出背景\n",
    "\n",
    "- 一般来说，TF-IDF算法和TextRank算法就能满足大部分关键词提取的任务。但是在某些场景，基于文档本身的关键词提取还不是非常足够，有些关键词并不一定会显式地出现在文档中，如一篇讲动物生存环境的科普文，可能通篇在介绍狮子、老虎等，但是都没有出现动物二字，这种情况下，前面两种算法显然不能提取出动物这个隐含的主题信息，这时候就需要用到主题模型。\n",
    "\n",
    "### 相关算法\n",
    "\n",
    "- LSA/LSI算法\n",
    "\n",
    "还有一种改进算法pLSA\n",
    "\n",
    "<font color=red>需要搞明白算法思想</font>\n",
    "\n",
    "- LDA算法\n",
    "\n",
    "<font color=red>需要搞明白算法思想</font>\n",
    "\n",
    "\n",
    "## 文本关键词提取实战\n",
    "\n",
    "- TF-IDF训练、LSI训练和LDA训练各有特点。TF-IDF的训练主要是根据数据集生成对应的IDF值字典，后续计算每个词的TF-IDF时，直接从字典中读取。LSI和LDA的训练是根据现有的数据集生成文档 - 主题分布矩阵和主题 - 词分布矩阵。\n",
    "\n",
    "- 一般情况下，使用词性过滤，仅保留名词作为关键字的结果更符合我们的要求，但是有些场景对其他词性的词有特殊的要求，可以根据场景的不同，选择需要过滤的不同词性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句法分析\n",
    "\n",
    "## 句法分析概述\n",
    "\n",
    "### 主要目标\n",
    "\n",
    "- 句法分析是自然语言处理的核心技术，是对语言进行深层次理解的基石。\n",
    "\n",
    "- 句法分析的主要任务是识别出句子所包含的句法成分以及这些成分之间的关系，一般以句法树来表示句法分析的结果。\n",
    "\n",
    "### 两大难点\n",
    "\n",
    "- 歧义\n",
    "\n",
    "> 自然语言区别于人工语言的一个重要特点就是它存在大量的歧义现象。\n",
    "\n",
    "- 搜索空间\n",
    "\n",
    "> 句法分析是一个极其复杂的任务，候选树个数随句子的增多呈指数级增长，搜索空间巨大。因此，必须设计合适的解码器，以确保能够在可容忍的时间内搜索到模型定义的最优解\n",
    "\n",
    "### 句法分析器\n",
    "\n",
    "- 句法分析是从单词串到句法结构的过程，而实现该过程的工具或程序称为句法分析器。\n",
    "\n",
    "### 分类\n",
    "\n",
    "- 句法分析的种类很多，根据侧重的目标，可分为完全句法分析和局部句法分析。两者的差别在于：\n",
    "\n",
    "> 完全句法分析以获得整个句子的句法结构为目的\n",
    "\n",
    "> 而局部句法分析只关注局部的一些成分，例如常用的依存句法分析，就属于局部句法分析。\n",
    "\n",
    "### 分析方法\n",
    "\n",
    "#### 基于规则的分析方法\n",
    "\n",
    "- 存在语法规则覆盖有限，系统可迁移性差等缺陷。\n",
    "\n",
    "#### 基于统计的分析方法\n",
    "\n",
    "- 随着大规模语料库的建立，基于统计学习模型的句法分析方法开始兴起，句法分析器的性能不断提高，最典型的就是风靡于20世纪70年代的PCFG(Probabilistic Context Free Grammar), 它在句法分析领域得到了极大的应用，也是现在句法分析中常用的方法。\n",
    "\n",
    "## 句法分析的数据集与评测方法\n",
    "\n",
    "- 统计分析方法一般都离不开语料数据集和相应的评价体系的支撑。\n",
    "\n",
    "### 句法分析的数据集\n",
    "\n",
    "- 常见树库\n",
    "\n",
    "> 英文宾州树库 PTB \n",
    "\n",
    "> 中文宾州树库 CTB\n",
    "\n",
    "> 清华树库 TCT\n",
    "\n",
    "> 台湾中研院树库\n",
    "\n",
    "### 句法分析的评测方法\n",
    "\n",
    "- 句法分析评测的主要任务是评测句法分析器生成的树结构与手工标注的树结构之间的相似度。其主要考虑两方面的性能：满意度和效率。\n",
    "\n",
    "> 满意度：是指测试句法分析器是否适合或胜任某个特定的自然语言处理任务\n",
    "\n",
    "> 效率：主要用于对比句法分析器的运行时间\n",
    "\n",
    "- 目前主流的句法分析评测方法是PARSEVAL评价体系，它是一种粒度比较适中，较为理想的评级方法，主要指标有准确率、召回率、交叉括号数。\n",
    "\n",
    "> 准确率：表示分析正确的短语个数在句法分析结果中所占的比例，即分析结果中与标准句法树中相匹配的短语个数占分析结果中所有短语个数的比例。\n",
    "\n",
    "> 召回率：表示分析得到的正确短语个数占标准分析树全部短语个数的比例。\n",
    "\n",
    "> 交叉括号数：表示分析得到的某一个短语的覆盖范围与标准句法分析结果的某个短语的覆盖范围存在重叠又不存在包含关系，即构成了一个交叉括号。\n",
    "\n",
    "\n",
    "## 句法分析的常用方法\n",
    "\n",
    "### 基于PCFG的句法分析\n",
    "\n",
    "- <font color = 'red'>基本思想及原理</font>\n",
    "\n",
    "### 基于最大间隔马尔可夫网络的句法分析\n",
    "\n",
    "- <font color=red>基本思想及原理</font>\n",
    "\n",
    "### 基于CRF的句法分析\n",
    "\n",
    "- <font color=red>基本思想及原理</font>\n",
    "\n",
    "### 基于移进-归约的句法分析模型\n",
    "\n",
    "- <font color=red>基本思想及原理</font>\n",
    "\n",
    "## 使用Stanford Parser 的PCFG算法进行句法分析\n",
    "\n",
    "- 句法分析算法实际性能离真正实用化还有不小的距离，主要原因在于，在语言学理论和实际的自然语言应用之间还存在着巨大的差距。\n",
    "\n",
    "- 句法分析常常通过结合一定的规则来辅助解决一些任务。如模板解析类的任务，可以通过句法分析进行语义标注提取其中的一些主谓宾关系，然后通过规则模板标出重要的角色信息和行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本向量化\n",
    "\n",
    "- 这是很重要的一个内容\n",
    "\n",
    "- 文本向量化的方法有很多，从之前基于统计的方法【Bow】，到时下流行的基于神经网络的方法，掌握word2vec词向量算法和doc2vec文本向量化算法是学习文本向量化的好方式。\n",
    "\n",
    "## 文本向量化概述\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 文本向量化就是将文本表示成一系列能够表达文本语义的向量。\n",
    "\n",
    "### 研究现状\n",
    "\n",
    "- 当前阶段，对文本向量化大部分的研究都是通过词向量化【word2vec】实现的。\n",
    "\n",
    "- 与此同时，也有相当一部分研究者将文章或者句子作为文本处理的基本单元，于是产生了doc2vec和str2vec技术。\n",
    "\n",
    "## <font color= 'red'>向量化算法word2vec </font>\n",
    "\n",
    "### 基于统计的文本向量化方式\n",
    "\n",
    "词袋模型【Bow】\n",
    "\n",
    "- 基本概念\n",
    "\n",
    "> 词袋模型是最早的以词语为基本处理单元的文本向量化方法。\n",
    "\n",
    "- 基本原理\n",
    "\n",
    "> 基于文档构建字典\n",
    "\n",
    "> 基于单词索引构建向量，该向量与原文本中单词出现的顺序没有关系，而是字典中每个单词在文本中出现的频率。\n",
    "\n",
    "\n",
    "- 缺点\n",
    "\n",
    "> 维度灾难\n",
    "\n",
    "> 无法保留词序信息\n",
    "\n",
    "> 存在语义鸿沟的问题\n",
    "\n",
    "\n",
    "### 基于分布式表征的文本向量化方式\n",
    "\n",
    "- 一般来说，词语是表达语义的基本单元，因为词袋模型只是将词语符号化，所以词语模型是不包含任何语义信息的。如何使“词表示”包含语义信息是该领域研究者面临的问题。\n",
    "\n",
    "- 分布式假说的提出为解决上述问题提供了理论基础。该假说的核心思想是：上下文相似的词，其语义也相似。\n",
    "\n",
    "- 随后有学者整理了利用上下文分布表示词义的方法，这类方法就是有名的词空间模型。\n",
    "\n",
    "#### 三种常见的生成词向量的神经网路模型\n",
    "\n",
    "- <font color = 'red'> 神经网络语言模型 NNLM </font>\n",
    "\n",
    "> 基本结构：三层网络结构 - 输入层、隐藏层、 输出层\n",
    "\n",
    "- <font color = 'red'> C&W模型 </font>\n",
    "\n",
    "> 基本结构： 三层网络结构 - 输入层、隐藏层、输出层\n",
    "\n",
    "> 与NNLM不同的是，输出层只有一个节点，表示n元语法的打分高低\n",
    "\n",
    "- <font color = 'red'> CBOW模型和Skip-gram模型 </font>\n",
    "\n",
    "> CBOW(Continuous Bag of-Words) 模型\n",
    "\n",
    "该模型只有两层，输出层和输出层。\n",
    "\n",
    "> Skip-gram模型\n",
    "\n",
    "该模型也是只有两层，输入层和输出层\n",
    "\n",
    "> CBOW和Skip-gram 实际上是word2vec两种不同思想的实现，CBOW的目标是根据上下文来预测当前词语的概率，且上下文所有的词对当前词出现概率的影响的权重是一样的，因此叫做Continuous Bag of-Words 模型。如在袋子中取词，取出数量足够的词就可以了，取出的先后顺序则是无关紧要的。\n",
    "\n",
    "> Skip-gram 刚好相反，其是根据当前词语来预测上下文概率的。\n",
    "\n",
    "\n",
    "## <font color = 'red'>向量化算法doc2vec/str2vec/para2vec </font>\n",
    "\n",
    "- 这三个说的是一个东西，别名\n",
    "\n",
    "### 提出背景\n",
    "\n",
    "- word2vec基于分布假说理论，可以很好地提取词语的语义信息，因此，利用word2vec技术计算词语间的相似度有非常好的效果。\n",
    "\n",
    "- 同样word2vec技术也用于计算句子或其他长文本间的相似度，其一般做法是对文本分词后，提取其关键词，用词向量表示这些关键词，接着对这些关键词向量求平均或者将其拼接，得到文本的词向量，最后利用该词向量计算文本间的相似度。\n",
    "\n",
    "- 这种方法虽然可行，但是丢失了文本中的语序信息，而通常来说，文本的语序包含重要信息。\n",
    "\n",
    "- 为了充分利用语序信息，有研究者在word2vec的基础上提出了文本向量化doc2vec，又称为str2vec 和 para2vec。\n",
    "\n",
    "### 基本原理\n",
    "\n",
    "- doc2vec技术存在两种模型, DM(Distributed Memory)模型和DBOW(Distributed Bag of words),分别对应word2vec技术中的CBOW和Skip-gram模型。\n",
    "\n",
    "> 与CBOW模型类似，DM模型试图预测给定上下文中某单词出现的频率，只不过DM模型的上下文不仅包括上下文单词而且还包括相应的段落。\n",
    "\n",
    "> DBOW则在给定段落向量的情况下，预测段落中一组随机单词的概率，与Skip-gram模型只给定一个语句预测目标词概率分布类似。\n",
    " \n",
    "## 小结\n",
    "\n",
    "- DM模型和DBOW模型相对应，故可根据上下文词向量和段向量预测目标词的概率分布。\n",
    "\n",
    "- DBOW模型和Skip-gram模型相对应，只输入段向量，预测从段落中随机抽取的词组概率分布。\n",
    "\n",
    "- 总体而言，doc2vec是word2vec的升级，doc2vec不仅提取了文本的语义信息，而且提取了文本的语序信息。\n",
    "\n",
    "\n",
    "## 将网页文本向量化实战\n",
    "\n",
    "### 训练词向量\n",
    "\n",
    "### 训练段向量\n",
    "\n",
    "### 利用word2vec 和 doc2vec计算网页相似度\n",
    "\n",
    "#### word2vec计算网页相似度步骤\n",
    "\n",
    "- 关键词提取\n",
    "\n",
    "- 关键词向量化\n",
    "\n",
    "- 相似度计算\n",
    "\n",
    "#### doc2vec计算网页相似度步骤\n",
    "\n",
    "- 预处理\n",
    "\n",
    "- 文本向量化\n",
    "\n",
    "- 计算文本相似度\n",
    "\n",
    "通常情况下，在长文本相似度计算的问题上，doc2vec方法更胜一筹，这是因为doc2vec不仅利用了词语的语义信息而且还综合了上下文语序信息，而word2vec则丢失了语序信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
