{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词理解\n",
    "\n",
    "## 关键词提取算法\n",
    "\n",
    "### TF-IDF \n",
    "\n",
    "- 详见有道云笔记\n",
    "\n",
    "### TextRank\n",
    "\n",
    "- 详见有道云笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子理解\n",
    "\n",
    "- 比较推荐的一款工具：LTP\n",
    "\n",
    "## 分句\n",
    "\n",
    "### 输入\n",
    "\n",
    "- 句子或篇章\n",
    "\n",
    "### 输出\n",
    "\n",
    "- 单个句子的序列\n",
    "\n",
    "### 示例\n",
    " \n",
    "```Python\n",
    "ltp.sent_split\n",
    "```\n",
    "\n",
    "## 分词\n",
    "\n",
    "- 中文语言的特有需求\n",
    "\n",
    "- 相对于分句，分词更重要，因为词是语言分析的基本单元\n",
    "\n",
    "- 针对一篇文章，可以不做分句处理，直接分词，但是反过来不行\n",
    "\n",
    "### 输入\n",
    "\n",
    "- 汉语句子\n",
    "\n",
    "### 输出\n",
    "\n",
    "- 词序列\n",
    "\n",
    "### 示例\n",
    "\n",
    "```Python\n",
    "segment, hidden = ltp.seg()\n",
    "segment\n",
    "```\n",
    "\n",
    "## 词性识别\n",
    "\n",
    "- 识别单词的词性，比如动词、名词、形容词\n",
    "\n",
    "- ltp 识别的词性貌似比jieba种类多\n",
    "\n",
    "### 输入\n",
    "\n",
    "- 句子\n",
    "\n",
    "### 输出\n",
    "\n",
    "- 识别的词性\n",
    "\n",
    "### 示例\n",
    "\n",
    "```Python\n",
    "segment, hidden = ltp.seg()\n",
    "ltp.pos(hidden)\n",
    "```\n",
    "\n",
    "## 命名实体识别\n",
    "\n",
    "- 识别词序列中的人名、地名、组织机构名\n",
    "\n",
    "### 输入\n",
    "\n",
    "- 句子\n",
    "\n",
    "### 输出\n",
    "\n",
    "- 命名实体识别结果\n",
    "\n",
    "### 示例\n",
    "\n",
    "```Python\n",
    "ltp.ner(hidden)\n",
    "```\n",
    "\n",
    "## 依存句法分析\n",
    "\n",
    "- 识别句子中的依存关系，比如主谓关系、动宾关系等\n",
    "\n",
    "- 最终形成的是一颗句法树，存在根节点\n",
    "\n",
    "### 输入\n",
    "\n",
    "- 句子\n",
    "\n",
    "### 输出\n",
    "\n",
    "- 依存tag\n",
    "\n",
    "### 示例\n",
    "\n",
    "```Python\n",
    "ltp.dep(hidden)\n",
    "```\n",
    "\n",
    "## 序列标注\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 序列标注问题是NLP领域中最常见的问题，因为绝大多数NLP问题都可以转化为序列标注问题\n",
    "\n",
    "- 虽然很多NLP任务看上去大不相同，但是如果转化为序列标注问题后其实面临的都是同一个问题\n",
    "\n",
    "- 所谓“序列标注”，就是指对于一个一维线性输入序列 $ X = x_1, x_2 \\ldots x_n $\n",
    "\n",
    "- 给线性序列中的每一个元素打上标签集合 $ Y = y_1, y_2 \\ldots y_n $ 中的某个或某几个标签\n",
    "\n",
    "- 所以， 其本质上就是对线性序列中的每一个元素根据上下文内容进行分类的问题\n",
    "\n",
    "- 即，解决如何根据汉字的上下文，给汉字打上一个或多个合适标签的问题， 分词、词性标注【分词其实就是给分词的位置打标】\n",
    "\n",
    "### 基础原理\n",
    "\n",
    "- 序列标注的基础原理是概率图模型，概率图模型，分为贝叶斯图（网络）和马尔科夫图（网络）。\n",
    "\n",
    "- 其中，贝叶斯网络可以用一个有向图结构表示，马尔科夫网络可以用无向图结构表示。\n",
    "\n",
    "- 其中，马尔科夫网络更偏向于展示节点之间的相互影响，而贝叶斯网络展示的是一个受另外一个或几个节点的影响\n",
    "\n",
    "#### 贝叶斯图\n",
    "\n",
    "##### 前置知识\n",
    "\n",
    "- 先验概率\n",
    "\n",
    "> 是指根据以往经验和分析得到的概率，如全概率公式\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "> 先验概率往往作为“由因求果”中的因\n",
    "\n",
    "- 比较吸引我的一个点是\n",
    "\n",
    "> 维基百科中关于全概率公式的解释，也是一种分而治之的思想，即将一个复杂事件A发生的概率求解问题，转化为在不同情况或者不同原因$B_n$下发生的简单事件的概率求和问题。\n",
    "\n",
    "- 后验概率\n",
    "\n",
    "> 是指依据得到“结果”信息所计算出的最有可能是哪种事件发生，比如已知头疼的信息下，推测最有可能是哪种事件引起的，如发烧？、被人打？积食？。如贝叶斯公式\n",
    "\n",
    "> 后验概率往往作为“执果寻因”中的因\n",
    "\n",
    "> 经验概率就是啥都知道了【从经验中知道的】，后验概率就是你知道结果了，完后去推测，引起这个结果的最大可能事件是什么\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "##### 基本概念\n",
    "\n",
    "> 贝叶斯网络，又称信念网络，或有向无环图模型，是一种概率图模型。\n",
    "\n",
    "> 它是一种模拟人类推理过程中的因果关系的不确定性处理模型，其网络拓扑结构是一个有向无环图（DAG）\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "> 贝叶斯网络的有向无环图中的节点表示随机变量{X1,X2,...,Xn}\n",
    "\n",
    "> 它们可以是可观察到的变量，或隐变量、未知参数等。认为有因果关系（或非条件独立）的变量或命题则用箭头来连接。若两个节点间以一个单箭头连接在一起，表示其中一个节点是“因(parents)”，另一个是“果(children)”，两节点就会产生一个条件概率值。\n",
    "\n",
    "- 总之\n",
    "\n",
    "> 贝叶斯网络就是将研究系统中涉及的随机变量，根据是否条件独立绘制在一个有向图中，就形成了贝叶斯网络\n",
    "\n",
    "> 其主要用来描述随机变量之间的条件依赖，节点表示随机变量，有向边表示依赖\n",
    "\n",
    "> 对于任意的随机变量，其联合概率可由各自的局部条件概率分布相乘而得出：\n",
    "\n",
    "> P(x1,...,xk)=P(xk|x1,...,xk−1)...P(x2|x1)P(x1)\n",
    "\n",
    "##### 网络结构\n",
    "\n",
    "- head-to-head\n",
    "\n",
    "- tail-to-tail\n",
    "\n",
    "- head-to-tail\n",
    "\n",
    "> 具体内容参考博文\n",
    "\n",
    "- https://www.cnblogs.com/mantch/p/11179933.html\n",
    "\n",
    "##### 额外补充\n",
    "\n",
    "- 相关公式\n",
    "\n",
    "> 贝叶斯公式\n",
    "\n",
    "![image-6.png](attachment:image-6.png)\n",
    "\n",
    "> 离散变量概率求解公式\n",
    "\n",
    "![image-7.png](attachment:image-7.png)\n",
    "\n",
    "> 连续变量概率求解公式\n",
    "\n",
    "![image-8.png](attachment:image-8.png)\n",
    "\n",
    "#### 马尔科夫图\n",
    "\n",
    "- 也叫马尔科夫随机场\n",
    "\n",
    "- 马尔科夫特征：在已知“现在”的条件下，“未来”和“过去”彼此独立的特性，就是马尔科夫特征\n",
    "\n",
    "##### 马尔科夫链\n",
    "\n",
    "- 马尔科夫链就是概率传导，即从一个初始概率传导到最终的概率，其中现在的概率取决于过去，且仅取决于最近的一次。\n",
    "\n",
    "- 链式法则\n",
    "\n",
    "![image-9.png](attachment:image-9.png)\n",
    "\n",
    "- 基本性质\n",
    "\n",
    "> 给定现在变量的时候，过去和未来这两个变量是相互独立的【看上面马尔科夫性质】\n",
    "\n",
    "> 是贝叶斯网络中head-to-tail 的一种特殊情况【即当前状态只跟上一状态有关】\n",
    "\n",
    "![image-10.png](attachment:image-10.png)\n",
    "\n",
    "\n",
    "##### 马尔科夫网络\n",
    "\n",
    "- 马尔可夫网络类似贝叶斯网络用于表示依赖关系。但是，一方面它可以表示贝叶斯网络无法表示的一些依赖关系，如循环依赖；另一方面，它不能表示贝叶斯网络能够表示的某些关系，如推导关系。\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/102014899\n",
    "\n",
    "> https://blog.csdn.net/zhuqiuhui/article/details/43267607\n",
    "\n",
    "- 自我感受\n",
    "\n",
    "> 目前，对这个马尔科夫网络还是比较模糊，是个无向可能有环图。但是主要是用来做什么的？能说明什么问题？\n",
    "\n",
    "\n",
    "##### 条件随机场\n",
    "\n",
    "- 现在晕了，回头再看\n",
    "\n",
    "### 基本方法\n",
    "\n",
    "#### HMM隐马尔科夫模型\n",
    "\n",
    "#### CRF条件随机场\n",
    "\n",
    "#### BiLSTM  + CRF\n",
    "\n",
    "#### Lattice + LSTM + CRF\n",
    "\n",
    "#### BERT + CRF\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> 概要式的介绍了序列标注方法： https://zhuanlan.zhihu.com/p/56317740\n",
    "\n",
    "> 详细介绍了序列标注的相关算法：https://cloud.tencent.com/developer/article/1598228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题模型\n",
    "\n",
    "## 背景\n",
    "\n",
    "### 有监督主题模型\n",
    "\n",
    "- 需要人工标注具体主题\n",
    "\n",
    "### 无监督主题模型\n",
    "\n",
    "- 不需要人工标注，但是学习出来的也是聚类id,而不是具体主题名称\n",
    "\n",
    "## 模型\n",
    "\n",
    "### Unigram Model\n",
    "\n",
    "\n",
    "### Mixture of Unigram Model\n",
    "\n",
    "\n",
    "### LSA\n",
    "\n",
    "\n",
    "### PLSA\n",
    "\n",
    "\n",
    "### LDA\n",
    "\n",
    "\n",
    "## 参考博文\n",
    "\n",
    "- https://blog.csdn.net/v_JULY_v/article/details/41209515\n",
    "\n",
    "- https://blog.csdn.net/hohaizx/article/details/79490920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
